{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1Y4TiKp8sgAcSENBINgwu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahatasham5/image-to-video-/blob/main/ltx_model_i2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ENVIRONMENT SETUP (NO UPSCALING) ---\n",
        "!pip install --quiet torch torchvision\n",
        "!pip install -q torchsde einops diffusers accelerate av\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "\n",
        "%cd /content\n",
        "\n",
        "# --- Clone ComfyUI & Custom Nodes ---\n",
        "!git clone --branch ComfyUI_v0.3.34 https://github.com/Isi-dev/ComfyUI\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone --branch forHidream https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_LTXVideo\n",
        "\n",
        "# --- Install Node Requirements ---\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_LTXVideo\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "# --- Download Models (NO UPSCALERS) ---\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-distilled-GGUF/resolve/main/ltxv-13b-0.9.7-distilled-Q6_K.gguf -d /content/ComfyUI/models/diffusion_models -o ltxv-13b-0.9.7-distilled-Q6_K.gguf\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Isi99999/LTX-Video/resolve/main/t5xxl_fp8_e4m3fn_scaled.safetensors -d /content/ComfyUI/models/text_encoders -o t5xxl_fp8_e4m3fn_scaled.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-dev-GGUF/resolve/main/ltxv-13b-0.9.7-vae-BF16.safetensors -d /content/ComfyUI/models/vae -o ltxv-13b-0.9.7-vae-BF16.safetensors\n"
      ],
      "metadata": {
        "id": "yO3fVf-rDxok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ComfyUI"
      ],
      "metadata": {
        "id": "3fDt9UHIiJaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IN8kdpbwCiw2"
      },
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "# --- IMPORTS & PATH SETUP ---\n",
        "import torch, gc, sys, random, os, imageio\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display, HTML\n",
        "sys.path.insert(0, '/content/ComfyUI')\n",
        "\n",
        "from comfy import model_management\n",
        "\n",
        "from nodes import (\n",
        "    CheckpointLoaderSimple, CLIPLoader, CLIPTextEncode, VAELoader, VAEDecode, VAEDecodeTiled,\n",
        "    LoadImage, ImageScale, SaveImage\n",
        ")\n",
        "\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "from comfy_extras.nodes_custom_sampler import KSamplerSelect, SamplerCustom, RandomNoise\n",
        "from comfy_extras.nodes_lt import LTXVPreprocess, LTXVImgToVideo, LTXVScheduler, LTXVConditioning\n",
        "from custom_nodes.ComfyUI_LTXVideo.stg import STGGuiderAdvancedNode\n",
        "from custom_nodes.ComfyUI_LTXVideo.easy_samplers import LTXVBaseSampler\n",
        "\n",
        "# (NO UPSCALER modules imported)\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "    for obj in list(globals().values()):\n",
        "        if torch.is_tensor(obj) or (hasattr(obj, \"data\") and torch.is_tensor(obj.data)):\n",
        "            del obj\n",
        "    gc.collect()\n",
        "\n",
        "def upload_image():\n",
        "    from google.colab import files\n",
        "    import shutil\n",
        "    os.makedirs('/content/ComfyUI/input', exist_ok=True)\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        src_path = f'/content/ComfyUI/{filename}'\n",
        "        dest_path = f'/content/ComfyUI/input/{filename}'\n",
        "        shutil.move(src_path, dest_path)\n",
        "        print(f\"Image saved to: {dest_path}\")\n",
        "        return dest_path\n",
        "    return None\n",
        "\n",
        "def string_to_float(string):\n",
        "    float_list = [float(x.strip()) for x in string.split(',')]\n",
        "    return (float_list,)\n",
        "\n",
        "def float_to_sigmas(float_list):\n",
        "    return torch.tensor(float_list, dtype=torch.float32),\n",
        "\n",
        "def image_width_height(image):\n",
        "    if image.ndim == 4:\n",
        "        _, height, width, _ = image.shape\n",
        "    elif image.ndim == 3:\n",
        "        height, width, _ = image.shape\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported image shape: {image.shape}\")\n",
        "    return width, height\n",
        "\n",
        "def display_video(video_path):\n",
        "    from IPython.display import HTML\n",
        "    from base64 import b64encode\n",
        "    mp4 = open(video_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    display(HTML(f'<video width=\"512\" controls><source src=\"{data_url}\" type=\"video/mp4\"></video>'))\n",
        "\n",
        "def generate_video(\n",
        "    image_path: str = None,\n",
        "    positive_prompt: str = \"A red fox moving gracefully\",\n",
        "    negative_prompt: str = \"low quality, worst quality\",\n",
        "    width: int = 768,\n",
        "    height: int = 512,\n",
        "    seed: int = 0,\n",
        "    steps: int = 30,\n",
        "    cfg_scale: float = 2.05,\n",
        "    sampler_name: str = \"euler\",\n",
        "    length: int = 24,\n",
        "    fps: int = 24,\n",
        "    upscale_video: bool = False # <- will be ignored\n",
        "):\n",
        "    with torch.inference_mode():\n",
        "        unet_loader = UnetLoaderGGUF()\n",
        "        vae_loader = VAELoader()\n",
        "        checkpoint_loader = CheckpointLoaderSimple()\n",
        "        clip_loader = CLIPLoader()\n",
        "        clip_encode_positive = CLIPTextEncode()\n",
        "        clip_encode_negative = CLIPTextEncode()\n",
        "        load_image = LoadImage()\n",
        "        image_scaler = ImageScale()\n",
        "        save_node = SaveImage()\n",
        "        preprocess = LTXVPreprocess()\n",
        "        img_to_video = LTXVImgToVideo()\n",
        "        scheduler = LTXVScheduler()\n",
        "        sampler_select = KSamplerSelect()\n",
        "        random_noise = RandomNoise()\n",
        "        conditioning = LTXVConditioning()\n",
        "        sampler = SamplerCustom()\n",
        "        vae_decode = VAEDecode()\n",
        "        stg_guider_advanced = STGGuiderAdvancedNode()\n",
        "        ltxv_base_sampler = LTXVBaseSampler()\n",
        "        vae_decode_tiled = VAEDecodeTiled()\n",
        "\n",
        "        print(\"Loading Text_Encoder...\")\n",
        "        clip = clip_loader.load_clip(\"t5xxl_fp8_e4m3fn_scaled.safetensors\", \"ltxv\", \"default\")[0]\n",
        "\n",
        "        assert width % 32 == 0, \"Width must be divisible by 32\"\n",
        "        assert height % 32 == 0, \"Height must be divisible by 32\"\n",
        "\n",
        "        positive = clip_encode_positive.encode(clip, positive_prompt)[0]\n",
        "        negative = clip_encode_negative.encode(clip, negative_prompt)[0]\n",
        "\n",
        "        torch.save(positive, '/content/positive_embedding.pt')\n",
        "        torch.save(negative, '/content/negative_embedding.pt')\n",
        "\n",
        "        del clip\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        if image_path is None:\n",
        "            print(\"Please upload an image file:\")\n",
        "            image_path = upload_image()\n",
        "        if image_path is None:\n",
        "            print(\"No image uploaded!\")\n",
        "        loaded_image = load_image.load_image(image_path)[0]\n",
        "\n",
        "        width_int, height_int = image_width_height(loaded_image)\n",
        "        if width == 0 and height == 0:\n",
        "            if width_int > height_int:\n",
        "                width = 768\n",
        "                height = 512\n",
        "            elif width_int == height_int:\n",
        "                width = 512\n",
        "                height = 512\n",
        "            else:\n",
        "                width = 512\n",
        "                height = 768\n",
        "\n",
        "        print(\"Loading UNet model...\")\n",
        "        model = unet_loader.load_unet(\"ltxv-13b-0.9.7-distilled-Q6_K.gguf\")[0]\n",
        "\n",
        "        conditionedPositive, conditionedNegative = conditioning.append(positive, negative, 25.0)\n",
        "\n",
        "        guider = stg_guider_advanced.get_guider(\n",
        "            model, conditionedPositive, conditionedNegative, 0.997, True,\n",
        "            \"1.0, 0.9933, 0.9850, 0.9767, 0.9008, 0.6180\",\n",
        "            \"1,1,1,1,1,1\", \"0,0,0,0,0,0\", \"1, 1, 1, 1, 1, 1\", \"[25], [35], [35], [42], [42], [42]\"\n",
        "        )[0]\n",
        "\n",
        "        print(\"Loading VAE...\")\n",
        "        vae = vae_loader.load_vae(\"ltxv-13b-0.9.7-vae-BF16.safetensors\")[0]\n",
        "\n",
        "        selected_sampler = sampler_select.get_sampler(sampler_name)[0]\n",
        "        sigmas = float_to_sigmas(string_to_float(\"1.0000, 0.9937, 0.9875, 0.9812, 0.9750, 0.9094, 0.7250, 0.4219, 0.0\")[0])[0]\n",
        "        noise = random_noise.get_noise(seed)[0]\n",
        "\n",
        "        # Rescale to desired size (NO UPSCALING)\n",
        "        loaded_image = image_scaler.upscale(loaded_image, \"lanczos\", width, height, \"disabled\")[0]\n",
        "\n",
        "        try:\n",
        "            print(\"Generating video...\")\n",
        "            sampled = ltxv_base_sampler.sample(\n",
        "                model, vae, width, height, length, guider, selected_sampler,\n",
        "                sigmas, noise, optional_cond_images=loaded_image, optional_cond_indices=\"0\",\n",
        "                strength=0.8, crop=\"disabled\", crf=30, blur=1\n",
        "            )[0]\n",
        "            torch.save(sampled, '/content/sample_latents.pt')\n",
        "            print(\"Latent saved as /content/sample_latents.pt\")\n",
        "            del model, guider, noise\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "            print(\"Decoding latents...\")\n",
        "            decoded = vae_decode.decode(vae, sampled)[0].detach()\n",
        "            del vae, sampled\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "            # Rescale to original image size (NO UPSCALING)\n",
        "            decoded = image_scaler.upscale(decoded, \"lanczos\", width_int, height_int, \"disabled\")[0]\n",
        "\n",
        "            output_path = \"/content/output.mp4\"\n",
        "            frames_np = (decoded.cpu().numpy() * 255).astype(np.uint8)\n",
        "            del decoded\n",
        "            torch.cuda.empty_cache(); gc.collect()\n",
        "\n",
        "            with imageio.get_writer(output_path, fps=fps) as writer:\n",
        "                for frame in frames_np:\n",
        "                    writer.append_data(frame)\n",
        "\n",
        "            print(f\"\\nVideo generation complete! Displaying Video...\")\n",
        "            display_video(output_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during video generation: {str(e)}\")\n",
        "            raise\n",
        "        finally:\n",
        "            clear_gpu_memory()\n",
        "\n",
        "print(\"✅ Environment Setup Complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_uploaded = upload_image()\n",
        "\n",
        "display_upload = False  # Set to True to display uploaded image in notebook output\n",
        "if display_upload:\n",
        "    from IPython.display import Image as IPImage, display\n",
        "    if file_uploaded and file_uploaded.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        display(IPImage(filename=file_uploaded))\n",
        "    else:\n",
        "        print(\"The image format cannot be displayed.\")"
      ],
      "metadata": {
        "id": "uIEXpNbWFS5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt crafted for your scene\n",
        "positive_prompt = (\n",
        "    \"An anime-style cinematic scene of a lone young man with dark hair and a travel bag \"\n",
        "    \"walking towards the grand gates of a Japanese academy, golden sunlight streaming through the trees, \"\n",
        "    \"detailed background, hopeful and dramatic atmosphere, soft clouds in the sky, highly detailed, anime art\"\n",
        ")\n",
        "\n",
        "negative_prompt = \"low quality, blurry, poorly drawn, deformed, low resolution, bad anatomy\"\n",
        "\n",
        "# Now generate the video:\n",
        "generate_video(\n",
        "    image_path=file_uploaded,\n",
        "    positive_prompt=positive_prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=768,\n",
        "    height=512,\n",
        "    length=24,\n",
        "    fps=24,\n",
        "    seed=42,\n",
        "    steps=30,\n",
        "    upscale_video=False  # No upscaling\n",
        ")"
      ],
      "metadata": {
        "id": "Thpp1j1CF3pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_prompt = (\n",
        "    \"cinematic wide shot, a large gorilla enters the sunlit scene and warmly hugs a surprised young man, \"\n",
        "    \"soft dynamic shadows, emotional anime style, detailed background, flowing camera movement, \"\n",
        "    \"expressive lighting, dramatic moment, fluid animation, subtle particles in air\"\n",
        ")\n",
        "\n",
        "negative_prompt = (\n",
        "    \"low quality, blurry, poorly drawn, deformed, bad anatomy, disfigured, motion smear, \"\n",
        "    \"motion artifacts, fused fingers, weird hands, ugly\"\n",
        ")\n",
        "\n",
        "# Set for 8 seconds of animation (24fps × 8 + 1 = 193 frames)\n",
        "generate_video(\n",
        "    image_path=file_uploaded,\n",
        "    positive_prompt=positive_prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=768,\n",
        "    height=512,\n",
        "    length=193,  # <- longer video for visible animation!\n",
        "    fps=24,\n",
        "    seed=42,\n",
        "    steps=30,\n",
        "    upscale_video=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "wgy7oo08IluD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "suaib = upload_image()\n",
        "\n",
        "#file_uploaded = \"/content/ComfyUI/e8521f2e-49b0-42bd-9270-b705a6d763c5.jfif\"  # your uploaded image path\n",
        "\n",
        "positive_prompt = (\n",
        "    \"two young men standing close together, smiling warmly and looking at each other with romantic look, \"\n",
        "    \"friendly atmosphere, candid selfie moment, Happy together, \"\n",
        "    \"well-groomed, natural expressions, clean background, sharp focus, realistic style, positive mood\"\n",
        ")\n",
        "\n",
        "negative_prompt = (\n",
        "    \"low quality, blurry, awkward pose, looking away, distorted faces, bad anatomy, poorly drawn hands, overexposed, cartoon, illustration\"\n",
        ")\n",
        "generate_video(\n",
        "    image_path=suaib,\n",
        "    positive_prompt=positive_prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=768,\n",
        "    height=768,         # square to fit your image shape\n",
        "    length=97,          # ~2 seconds of animation at 24fps = 49\n",
        "    fps=24,\n",
        "    seed=77,            # any number you like\n",
        "    steps=30,\n",
        "    upscale_video=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "tE1rljMvLKn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For upscaling\n",
        "# New Section"
      ],
      "metadata": {
        "id": "mQ5WLI1lJnsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for upscalling\n",
        "import torch\n",
        "torch.save(sampled, '/content/sample_latents.pt')\n"
      ],
      "metadata": {
        "id": "YssruFrFHelg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -- Install dependencies --\n",
        "!pip install --quiet torch torchvision torchsde einops diffusers accelerate av\n",
        "\n",
        "# -- System packages --\n",
        "!apt -y install -qq aria2 ffmpeg\n",
        "\n",
        "# -- Clone ComfyUI and custom nodes --\n",
        "%cd /content\n",
        "!git clone --branch ComfyUI_v0.3.34 https://github.com/Isi-dev/ComfyUI\n",
        "%cd /content/ComfyUI/custom_nodes\n",
        "!git clone --branch forHidream https://github.com/Isi-dev/ComfyUI_GGUF.git\n",
        "!git clone https://github.com/Isi-dev/ComfyUI_LTXVideo\n",
        "\n",
        "# -- Install requirements for nodes --\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_GGUF\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI/custom_nodes/ComfyUI_LTXVideo\n",
        "!pip install -r requirements.txt\n",
        "%cd /content/ComfyUI\n",
        "\n",
        "# -- Download models (edit these if you have your own paths) --\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-distilled-GGUF/resolve/main/ltxv-13b-0.9.7-distilled-Q6_K.gguf -d /content/ComfyUI/models/diffusion_models -o ltxv-13b-0.9.7-distilled-Q6_K.gguf\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/wsbagnsv1/ltxv-13b-0.9.7-dev-GGUF/resolve/main/ltxv-13b-0.9.7-vae-BF16.safetensors -d /content/ComfyUI/models/vae -o ltxv-13b-0.9.7-vae-BF16.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-spatial-upscaler-0.9.7.safetensors -d /content/ComfyUI/models/upscale_models -o ltxv-spatial-upscaler-0.9.7.safetensors\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "# Now upload your latent file if you haven't already (e.g., via Colab file upload UI)\n"
      ],
      "metadata": {
        "id": "Wiojgl9jPOId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltxv-spatial-upscaler-0.9.7.safetensors -d /content/ComfyUI/models/upscale_models -o ltxv-spatial-upscaler-0.9.7.safetensors"
      ],
      "metadata": {
        "id": "II_YVhX6Yv3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "from comfy import model_management\n",
        "\n",
        "# Re-import all needed upscaler and utility modules\n",
        "from custom_nodes.ComfyUI_GGUF.nodes import UnetLoaderGGUF\n",
        "from nodes import VAELoader, VAEDecodeTiled, ImageScale\n",
        "from comfy_extras.nodes_custom_sampler import RandomNoise\n",
        "from custom_nodes.ComfyUI_LTXVideo.stg import STGGuiderAdvancedNode\n",
        "from custom_nodes.ComfyUI_LTXVideo.latent_upsampler import (\n",
        "    LTXVLatentUpsamplerModelLoader, LTXVLatentUpsampler\n",
        ")\n",
        "from custom_nodes.ComfyUI_LTXVideo.latent_adain import LTXVAdainLatent\n",
        "from custom_nodes.ComfyUI_LTXVideo.tiled_sampler import LTXVTiledSampler\n",
        "\n",
        "# --- Load your sampled latents ---\n",
        "latents = torch.load('/content/sample_latents.pt', map_location='cuda')  # or 'cpu' if you don't have a GPU\n",
        "\n",
        "# --- Model and upscaler loading (paths must match your session/setup) ---\n",
        "unet_loader = UnetLoaderGGUF()\n",
        "vae_loader = VAELoader()\n",
        "vae_decode_tiled = VAEDecodeTiled()\n",
        "image_scaler = ImageScale()\n",
        "upscale_model_loader = LTXVLatentUpsamplerModelLoader()\n",
        "latent_upsampler = LTXVLatentUpsampler()\n",
        "adain_latent = LTXVAdainLatent()\n",
        "tiled_sampler = LTXVTiledSampler()\n",
        "random_noise = RandomNoise()\n",
        "stg_guider_advanced = STGGuiderAdvancedNode()\n",
        "\n",
        "# Load UNet, VAE, Upscale models (use the same model names as in the original code)\n",
        "model = unet_loader.load_unet(\"ltxv-13b-0.9.7-distilled-Q6_K.gguf\")[0]\n",
        "vae = vae_loader.load_vae(\"ltxv-13b-0.9.7-vae-BF16.safetensors\")[0]\n",
        "upscale_model = upscale_model_loader.load_model(\n",
        "    \"ltxv-spatial-upscaler-0.9.7.safetensors\", True, False\n",
        ")[0]\n",
        "\n",
        "# Provide the same positive/negative conditioning as original (you may need to reload text encodings or save them in step 1)\n",
        "# For brevity, I'll use dummy values:\n",
        "conditionedPositive = torch.zeros((1,))  # <-- Replace with your original values!\n",
        "conditionedNegative = torch.zeros((1,))\n",
        "\n",
        "conditionedPositive = torch.load('/content/positive_embedding.pt', map_location='cuda')\n",
        "conditionedNegative = torch.load('/content/negative_embedding.pt', map_location='cuda')\n",
        "\n",
        "\n",
        "# Also, you’ll need to set the guider and other parameters just as in your original upscaling block:\n",
        "tiled_guider = stg_guider_advanced.get_guider(\n",
        "    model, conditionedPositive, conditionedNegative,\n",
        "    0.997, True, \"1\", \"1\", \"0\", \"1\", \"[42]\"\n",
        ")[0]\n",
        "\n",
        "import numpy as np\n",
        "def string_to_float(string):\n",
        "    float_list = [float(x.strip()) for x in string.split(',')]\n",
        "    return (float_list,)\n",
        "\n",
        "def float_to_sigmas(float_list):\n",
        "    return torch.tensor(float_list, dtype=torch.float32),\n",
        "\n",
        "tiled_sigmas = float_to_sigmas(\n",
        "    string_to_float(\"0.85, 0.7250, 0.6, 0.4219, 0.0\")[0]\n",
        ")[0]\n",
        "\n",
        "# --- Actual upscaling pipeline (copied from your original code) ---\n",
        "upscaled_latents = latent_upsampler.upsample_latent(\n",
        "    latents, upscale_model, vae\n",
        ")[0]\n",
        "\n",
        "adjusted_latents = adain_latent.batch_normalize(\n",
        "    upscaled_latents, latents, 0.25\n",
        ")[0]\n",
        "\n",
        "# Release models as needed\n",
        "del latents, upscale_model, upscaled_latents\n",
        "gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Generate noise for tiling\n",
        "seed = 77  # use same seed as before!\n",
        "tiled_noise = random_noise.get_noise(seed)[0]\n",
        "\n",
        "# Specify your width/height/etc. as needed, or restore from metadata\n",
        "width = 512\n",
        "height = 512\n",
        "\n",
        "# The `loaded_image` should match the conditions (may use a dummy/zeros if not using cond images)\n",
        "loaded_image = torch.zeros((1, height, width, 3), dtype=torch.float32) # adjust as needed\n",
        "\n",
        "# Tiled sampling\n",
        "tiled_output, _ = tiled_sampler.sample(\n",
        "    model=model,\n",
        "    vae=vae,\n",
        "    noise=tiled_noise,\n",
        "    sampler=None,  # supply your sampler\n",
        "    sigmas=tiled_sigmas,\n",
        "    guider=tiled_guider,\n",
        "    latents=adjusted_latents,\n",
        "    optional_cond_images=loaded_image,\n",
        "    horizontal_tiles=1,\n",
        "    vertical_tiles=1,\n",
        "    overlap=1,\n",
        "    latents_cond_strength=0.15,\n",
        "    boost_latent_similarity=False,\n",
        "    crop=\"disabled\",\n",
        "    optional_cond_indices=\"0\",\n",
        "    images_cond_strengths=\"0.9\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# --- After tiled_output ---\n",
        "upscaled_latents = tiled_output[\"samples\"]\n",
        "latent_input = { \"samples\": upscaled_latents }\n",
        "decoded_frames = vae_decode_tiled.decode(vae, latent_input, width, 64, 64, 8)[0]\n",
        "\n",
        "# Upscale to 1080p square (or 1920, 1080 for landscape)\n",
        "decoded_frames = image_scaler.upscale(decoded_frames, \"lanczos\", 1080, 1080, \"disabled\")[0]\n",
        "\n",
        "import imageio\n",
        "output_pathU = \"/content/upscaled.mp4\"\n",
        "frames_npu = (decoded_frames.cpu().numpy() * 255).astype(np.uint8)\n",
        "with imageio.get_writer(output_pathU, fps=12) as writer:  # Set FPS as you want\n",
        "    for frame in frames_npu:\n",
        "        writer.append_data(frame)\n",
        "\n",
        "print(\"Upscaled video saved to:\", output_pathU)\n"
      ],
      "metadata": {
        "id": "y9Uec-4rHm0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}